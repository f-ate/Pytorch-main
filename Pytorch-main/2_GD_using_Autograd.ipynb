{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350ec825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# f = w*x\n",
    "# f = 2*x\n",
    "\n",
    "X = np.array([1,2,3,4],dtype=np.float32)\n",
    "y = np.array([1,4,6,8],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb659fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11d806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e691c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64fb641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient\n",
    "#MSE = 1/N*(w*x-y)**2\n",
    "#dj/dw = 1/N 2x(w*x-y)\n",
    "\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edef05c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca41c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff44fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-118.0\n",
      "epoch 1: w =1.180,loss=29.25000000\n",
      "-47.2\n",
      "epoch 2: w =1.652,loss=4.88300037\n",
      "-18.880001\n",
      "epoch 3: w =1.841,loss=0.98428023\n",
      "-7.5519986\n",
      "epoch 4: w =1.916,loss=0.36048478\n",
      "-3.0208013\n",
      "epoch 5: w =1.947,loss=0.26067758\n",
      "-1.2083225\n",
      "epoch 6: w =1.959,loss=0.24470843\n",
      "-0.48332644\n",
      "epoch 7: w =1.963,loss=0.24215336\n",
      "-0.19333315\n",
      "epoch 8: w =1.965,loss=0.24174455\n",
      "-0.07733154\n",
      "epoch 9: w =1.966,loss=0.24167913\n",
      "-0.030933619\n",
      "epoch 10: w =1.966,loss=0.24166866\n",
      "-0.012370586\n",
      "epoch 11: w =1.967,loss=0.24166697\n",
      "-0.0049476624\n",
      "epoch 12: w =1.967,loss=0.24166672\n",
      "-0.0019800663\n",
      "epoch 13: w =1.967,loss=0.24166667\n",
      "-0.00079131126\n",
      "epoch 14: w =1.967,loss=0.24166666\n",
      "-0.000320673\n",
      "epoch 15: w =1.967,loss=0.24166666\n",
      "-0.00012540817\n",
      "epoch 16: w =1.967,loss=0.24166666\n",
      "-4.7445297e-05\n",
      "epoch 17: w =1.967,loss=0.24166666\n",
      "-1.8835068e-05\n",
      "epoch 18: w =1.967,loss=0.24166666\n",
      "-1.0967255e-05\n",
      "epoch 19: w =1.967,loss=0.24166664\n",
      "-5.9604645e-06\n",
      "epoch 20: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 21: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 22: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 23: w =1.967,loss=0.24166666\n",
      "-5.9604645e-06\n",
      "epoch 24: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 25: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 26: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 27: w =1.967,loss=0.24166666\n",
      "-5.9604645e-06\n",
      "epoch 28: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 29: w =1.967,loss=0.24166666\n",
      "1.9073486e-06\n",
      "epoch 30: w =1.967,loss=0.24166666\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(n_iters):\n",
    "    #forward\n",
    "    y_pred = forward(X)\n",
    "    #Loss\n",
    "    l = loss(y,y_pred)\n",
    "    #gradient\n",
    "    dw = gradient(X,y,y_pred)\n",
    "    print(dw)\n",
    "    # update weights\n",
    "    w -= learning_rate*dw\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b238498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d084fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y = torch.tensor([1,4,6,8],dtype=torch.float32)\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a8db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "print(f\"prediction before Training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56dae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a74837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.295,loss=29.25000000\n",
      "epoch 2: w =0.546,loss=21.20018768\n",
      "epoch 3: w =0.759,loss=15.38419724\n",
      "epoch 4: w =0.940,loss=11.18214512\n",
      "epoch 5: w =1.094,loss=8.14616203\n",
      "epoch 6: w =1.225,loss=5.95266438\n",
      "epoch 7: w =1.336,loss=4.36786270\n",
      "epoch 8: w =1.431,loss=3.22284269\n",
      "epoch 9: w =1.511,loss=2.39556646\n",
      "epoch 10: w =1.579,loss=1.79785860\n",
      "epoch 11: w =1.638,loss=1.36601555\n",
      "epoch 12: w =1.687,loss=1.05400896\n",
      "epoch 13: w =1.729,loss=0.82858384\n",
      "epoch 14: w =1.765,loss=0.66571426\n",
      "epoch 15: w =1.795,loss=0.54804134\n",
      "epoch 16: w =1.821,loss=0.46302229\n",
      "epoch 17: w =1.843,loss=0.40159622\n",
      "epoch 18: w =1.861,loss=0.35721567\n",
      "epoch 19: w =1.877,loss=0.32515085\n",
      "epoch 20: w =1.890,loss=0.30198389\n",
      "epoch 21: w =1.902,loss=0.28524587\n",
      "epoch 22: w =1.912,loss=0.27315259\n",
      "epoch 23: w =1.920,loss=0.26441529\n",
      "epoch 24: w =1.927,loss=0.25810257\n",
      "epoch 25: w =1.933,loss=0.25354165\n",
      "epoch 26: w =1.938,loss=0.25024632\n",
      "epoch 27: w =1.942,loss=0.24786550\n",
      "epoch 28: w =1.946,loss=0.24614532\n",
      "epoch 29: w =1.949,loss=0.24490248\n",
      "epoch 30: w =1.952,loss=0.24400455\n",
      "epoch 31: w =1.954,loss=0.24335578\n",
      "epoch 32: w =1.956,loss=0.24288708\n",
      "epoch 33: w =1.957,loss=0.24254841\n",
      "epoch 34: w =1.959,loss=0.24230374\n",
      "epoch 35: w =1.960,loss=0.24212694\n",
      "epoch 36: w =1.961,loss=0.24199921\n",
      "epoch 37: w =1.962,loss=0.24190693\n",
      "epoch 38: w =1.963,loss=0.24184027\n",
      "epoch 39: w =1.963,loss=0.24179210\n",
      "epoch 40: w =1.964,loss=0.24175730\n",
      "epoch 41: w =1.964,loss=0.24173214\n",
      "epoch 42: w =1.965,loss=0.24171396\n",
      "epoch 43: w =1.965,loss=0.24170083\n",
      "epoch 44: w =1.965,loss=0.24169137\n",
      "epoch 45: w =1.965,loss=0.24168451\n",
      "epoch 46: w =1.966,loss=0.24167953\n",
      "epoch 47: w =1.966,loss=0.24167597\n",
      "epoch 48: w =1.966,loss=0.24167341\n",
      "epoch 49: w =1.966,loss=0.24167153\n",
      "epoch 50: w =1.966,loss=0.24167019\n",
      "epoch 51: w =1.966,loss=0.24166919\n",
      "epoch 52: w =1.966,loss=0.24166849\n",
      "epoch 53: w =1.966,loss=0.24166799\n",
      "epoch 54: w =1.966,loss=0.24166763\n",
      "epoch 55: w =1.966,loss=0.24166735\n",
      "epoch 56: w =1.966,loss=0.24166717\n",
      "epoch 57: w =1.966,loss=0.24166702\n",
      "epoch 58: w =1.967,loss=0.24166693\n",
      "epoch 59: w =1.967,loss=0.24166687\n",
      "epoch 60: w =1.967,loss=0.24166679\n",
      "epoch 61: w =1.967,loss=0.24166678\n",
      "epoch 62: w =1.967,loss=0.24166675\n",
      "epoch 63: w =1.967,loss=0.24166672\n",
      "epoch 64: w =1.967,loss=0.24166670\n",
      "epoch 65: w =1.967,loss=0.24166670\n",
      "epoch 66: w =1.967,loss=0.24166667\n",
      "epoch 67: w =1.967,loss=0.24166667\n",
      "epoch 68: w =1.967,loss=0.24166669\n",
      "epoch 69: w =1.967,loss=0.24166666\n",
      "epoch 70: w =1.967,loss=0.24166667\n",
      "epoch 71: w =1.967,loss=0.24166670\n",
      "epoch 72: w =1.967,loss=0.24166666\n",
      "epoch 73: w =1.967,loss=0.24166667\n",
      "epoch 74: w =1.967,loss=0.24166669\n",
      "epoch 75: w =1.967,loss=0.24166666\n",
      "epoch 76: w =1.967,loss=0.24166667\n",
      "epoch 77: w =1.967,loss=0.24166667\n",
      "epoch 78: w =1.967,loss=0.24166666\n",
      "epoch 79: w =1.967,loss=0.24166666\n",
      "epoch 80: w =1.967,loss=0.24166666\n",
      "epoch 81: w =1.967,loss=0.24166667\n",
      "epoch 82: w =1.967,loss=0.24166666\n",
      "epoch 83: w =1.967,loss=0.24166666\n",
      "epoch 84: w =1.967,loss=0.24166667\n",
      "epoch 85: w =1.967,loss=0.24166666\n",
      "epoch 86: w =1.967,loss=0.24166666\n",
      "epoch 87: w =1.967,loss=0.24166667\n",
      "epoch 88: w =1.967,loss=0.24166667\n",
      "epoch 89: w =1.967,loss=0.24166666\n",
      "epoch 90: w =1.967,loss=0.24166667\n",
      "epoch 91: w =1.967,loss=0.24166666\n",
      "epoch 92: w =1.967,loss=0.24166666\n",
      "epoch 93: w =1.967,loss=0.24166667\n",
      "epoch 94: w =1.967,loss=0.24166667\n",
      "epoch 95: w =1.967,loss=0.24166667\n",
      "epoch 96: w =1.967,loss=0.24166666\n",
      "epoch 97: w =1.967,loss=0.24166666\n",
      "epoch 98: w =1.967,loss=0.24166666\n",
      "epoch 99: w =1.967,loss=0.24166666\n",
      "epoch 100: w =1.967,loss=0.24166666\n",
      "epoch 101: w =1.967,loss=0.24166666\n",
      "epoch 102: w =1.967,loss=0.24166666\n",
      "epoch 103: w =1.967,loss=0.24166666\n",
      "epoch 104: w =1.967,loss=0.24166666\n",
      "epoch 105: w =1.967,loss=0.24166666\n",
      "epoch 106: w =1.967,loss=0.24166666\n",
      "epoch 107: w =1.967,loss=0.24166666\n",
      "epoch 108: w =1.967,loss=0.24166666\n",
      "epoch 109: w =1.967,loss=0.24166666\n",
      "epoch 110: w =1.967,loss=0.24166666\n",
      "epoch 111: w =1.967,loss=0.24166666\n",
      "epoch 112: w =1.967,loss=0.24166666\n",
      "epoch 113: w =1.967,loss=0.24166666\n",
      "epoch 114: w =1.967,loss=0.24166666\n",
      "epoch 115: w =1.967,loss=0.24166666\n",
      "epoch 116: w =1.967,loss=0.24166666\n",
      "epoch 117: w =1.967,loss=0.24166666\n",
      "epoch 118: w =1.967,loss=0.24166666\n",
      "epoch 119: w =1.967,loss=0.24166666\n",
      "epoch 120: w =1.967,loss=0.24166666\n",
      "epoch 121: w =1.967,loss=0.24166666\n",
      "epoch 122: w =1.967,loss=0.24166666\n",
      "epoch 123: w =1.967,loss=0.24166666\n",
      "epoch 124: w =1.967,loss=0.24166666\n",
      "epoch 125: w =1.967,loss=0.24166666\n",
      "epoch 126: w =1.967,loss=0.24166666\n",
      "epoch 127: w =1.967,loss=0.24166666\n",
      "epoch 128: w =1.967,loss=0.24166666\n",
      "epoch 129: w =1.967,loss=0.24166666\n",
      "epoch 130: w =1.967,loss=0.24166666\n",
      "epoch 131: w =1.967,loss=0.24166666\n",
      "epoch 132: w =1.967,loss=0.24166666\n",
      "epoch 133: w =1.967,loss=0.24166666\n",
      "epoch 134: w =1.967,loss=0.24166666\n",
      "epoch 135: w =1.967,loss=0.24166666\n",
      "epoch 136: w =1.967,loss=0.24166666\n",
      "epoch 137: w =1.967,loss=0.24166666\n",
      "epoch 138: w =1.967,loss=0.24166666\n",
      "epoch 139: w =1.967,loss=0.24166666\n",
      "epoch 140: w =1.967,loss=0.24166666\n",
      "epoch 141: w =1.967,loss=0.24166666\n",
      "epoch 142: w =1.967,loss=0.24166666\n",
      "epoch 143: w =1.967,loss=0.24166666\n",
      "epoch 144: w =1.967,loss=0.24166666\n",
      "epoch 145: w =1.967,loss=0.24166666\n",
      "epoch 146: w =1.967,loss=0.24166666\n",
      "epoch 147: w =1.967,loss=0.24166666\n",
      "epoch 148: w =1.967,loss=0.24166666\n",
      "epoch 149: w =1.967,loss=0.24166666\n",
      "epoch 150: w =1.967,loss=0.24166666\n",
      "epoch 151: w =1.967,loss=0.24166666\n",
      "epoch 152: w =1.967,loss=0.24166666\n",
      "epoch 153: w =1.967,loss=0.24166666\n",
      "epoch 154: w =1.967,loss=0.24166666\n",
      "epoch 155: w =1.967,loss=0.24166666\n",
      "epoch 156: w =1.967,loss=0.24166666\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(y,y_pred)\n",
    "    # backword\n",
    "    l.backward()\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    #zero grad\n",
    "    w.grad.zero_()\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53ecbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b23be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae9bc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y = torch.tensor([1,4,6,8],dtype=torch.float32)\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c3f08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9be5a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32425b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac6ee2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.295,loss=29.25000000\n",
      "epoch 2: w =0.546,loss=21.20018768\n",
      "epoch 3: w =0.759,loss=15.38419914\n",
      "epoch 4: w =0.940,loss=11.18214607\n",
      "epoch 5: w =1.094,loss=8.14616203\n",
      "epoch 6: w =1.225,loss=5.95266438\n",
      "epoch 7: w =1.336,loss=4.36786270\n",
      "epoch 8: w =1.431,loss=3.22284269\n",
      "epoch 9: w =1.511,loss=2.39556646\n",
      "epoch 10: w =1.579,loss=1.79785860\n",
      "epoch 11: w =1.638,loss=1.36601555\n",
      "epoch 12: w =1.687,loss=1.05400896\n",
      "epoch 13: w =1.729,loss=0.82858384\n",
      "epoch 14: w =1.765,loss=0.66571426\n",
      "epoch 15: w =1.795,loss=0.54804134\n",
      "epoch 16: w =1.821,loss=0.46302229\n",
      "epoch 17: w =1.843,loss=0.40159622\n",
      "epoch 18: w =1.861,loss=0.35721567\n",
      "epoch 19: w =1.877,loss=0.32515085\n",
      "epoch 20: w =1.890,loss=0.30198389\n",
      "epoch 21: w =1.902,loss=0.28524587\n",
      "epoch 22: w =1.912,loss=0.27315259\n",
      "epoch 23: w =1.920,loss=0.26441529\n",
      "epoch 24: w =1.927,loss=0.25810257\n",
      "epoch 25: w =1.933,loss=0.25354165\n",
      "epoch 26: w =1.938,loss=0.25024632\n",
      "epoch 27: w =1.942,loss=0.24786550\n",
      "epoch 28: w =1.946,loss=0.24614532\n",
      "epoch 29: w =1.949,loss=0.24490248\n",
      "epoch 30: w =1.952,loss=0.24400455\n",
      "epoch 31: w =1.954,loss=0.24335578\n",
      "epoch 32: w =1.956,loss=0.24288708\n",
      "epoch 33: w =1.957,loss=0.24254841\n",
      "epoch 34: w =1.959,loss=0.24230374\n",
      "epoch 35: w =1.960,loss=0.24212694\n",
      "epoch 36: w =1.961,loss=0.24199921\n",
      "epoch 37: w =1.962,loss=0.24190693\n",
      "epoch 38: w =1.963,loss=0.24184027\n",
      "epoch 39: w =1.963,loss=0.24179210\n",
      "epoch 40: w =1.964,loss=0.24175730\n",
      "epoch 41: w =1.964,loss=0.24173214\n",
      "epoch 42: w =1.965,loss=0.24171396\n",
      "epoch 43: w =1.965,loss=0.24170083\n",
      "epoch 44: w =1.965,loss=0.24169137\n",
      "epoch 45: w =1.965,loss=0.24168451\n",
      "epoch 46: w =1.966,loss=0.24167953\n",
      "epoch 47: w =1.966,loss=0.24167597\n",
      "epoch 48: w =1.966,loss=0.24167341\n",
      "epoch 49: w =1.966,loss=0.24167153\n",
      "epoch 50: w =1.966,loss=0.24167019\n",
      "epoch 51: w =1.966,loss=0.24166919\n",
      "epoch 52: w =1.966,loss=0.24166849\n",
      "epoch 53: w =1.966,loss=0.24166799\n",
      "epoch 54: w =1.966,loss=0.24166763\n",
      "epoch 55: w =1.966,loss=0.24166735\n",
      "epoch 56: w =1.966,loss=0.24166717\n",
      "epoch 57: w =1.966,loss=0.24166702\n",
      "epoch 58: w =1.967,loss=0.24166693\n",
      "epoch 59: w =1.967,loss=0.24166687\n",
      "epoch 60: w =1.967,loss=0.24166679\n",
      "epoch 61: w =1.967,loss=0.24166678\n",
      "epoch 62: w =1.967,loss=0.24166675\n",
      "epoch 63: w =1.967,loss=0.24166672\n",
      "epoch 64: w =1.967,loss=0.24166670\n",
      "epoch 65: w =1.967,loss=0.24166670\n",
      "epoch 66: w =1.967,loss=0.24166667\n",
      "epoch 67: w =1.967,loss=0.24166667\n",
      "epoch 68: w =1.967,loss=0.24166669\n",
      "epoch 69: w =1.967,loss=0.24166666\n",
      "epoch 70: w =1.967,loss=0.24166667\n",
      "epoch 71: w =1.967,loss=0.24166670\n",
      "epoch 72: w =1.967,loss=0.24166666\n",
      "epoch 73: w =1.967,loss=0.24166667\n",
      "epoch 74: w =1.967,loss=0.24166669\n",
      "epoch 75: w =1.967,loss=0.24166666\n",
      "epoch 76: w =1.967,loss=0.24166667\n",
      "epoch 77: w =1.967,loss=0.24166667\n",
      "epoch 78: w =1.967,loss=0.24166666\n",
      "epoch 79: w =1.967,loss=0.24166666\n",
      "epoch 80: w =1.967,loss=0.24166666\n",
      "epoch 81: w =1.967,loss=0.24166667\n",
      "epoch 82: w =1.967,loss=0.24166666\n",
      "epoch 83: w =1.967,loss=0.24166666\n",
      "epoch 84: w =1.967,loss=0.24166667\n",
      "epoch 85: w =1.967,loss=0.24166666\n",
      "epoch 86: w =1.967,loss=0.24166666\n",
      "epoch 87: w =1.967,loss=0.24166667\n",
      "epoch 88: w =1.967,loss=0.24166667\n",
      "epoch 89: w =1.967,loss=0.24166666\n",
      "epoch 90: w =1.967,loss=0.24166667\n",
      "epoch 91: w =1.967,loss=0.24166666\n",
      "epoch 92: w =1.967,loss=0.24166666\n",
      "epoch 93: w =1.967,loss=0.24166667\n",
      "epoch 94: w =1.967,loss=0.24166667\n",
      "epoch 95: w =1.967,loss=0.24166667\n",
      "epoch 96: w =1.967,loss=0.24166666\n",
      "epoch 97: w =1.967,loss=0.24166666\n",
      "epoch 98: w =1.967,loss=0.24166666\n",
      "epoch 99: w =1.967,loss=0.24166666\n",
      "epoch 100: w =1.967,loss=0.24166666\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d1da5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4th way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e1bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[1],[4],[6],[8]],dtype=torch.float32)\n",
    "X_test = torch.tensor([5],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3235292a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165aa642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_features = X.shape\n",
    "print(n_samples)\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ab4d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c61f313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de59f7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=2.202\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361e0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3410501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.645,loss=17.41027832\n",
      "epoch 2: w =0.834,loss=12.22733688\n",
      "epoch 3: w =0.991,loss=8.63026333\n",
      "epoch 4: w =1.123,loss=6.13359451\n",
      "epoch 5: w =1.233,loss=4.40047789\n",
      "epoch 6: w =1.325,loss=3.19717884\n",
      "epoch 7: w =1.402,loss=2.36151147\n",
      "epoch 8: w =1.466,loss=1.78094220\n",
      "epoch 9: w =1.520,loss=1.37738347\n",
      "epoch 10: w =1.565,loss=1.09665358\n",
      "epoch 11: w =1.602,loss=0.90115517\n",
      "epoch 12: w =1.634,loss=0.76480228\n",
      "epoch 13: w =1.661,loss=0.66949254\n",
      "epoch 14: w =1.683,loss=0.60266721\n",
      "epoch 15: w =1.702,loss=0.55560982\n",
      "epoch 16: w =1.718,loss=0.52227300\n",
      "epoch 17: w =1.732,loss=0.49846107\n",
      "epoch 18: w =1.743,loss=0.48126209\n",
      "epoch 19: w =1.753,loss=0.46865615\n",
      "epoch 20: w =1.762,loss=0.45924094\n",
      "epoch 21: w =1.769,loss=0.45204347\n",
      "epoch 22: w =1.776,loss=0.44638923\n",
      "epoch 23: w =1.781,loss=0.44180971\n",
      "epoch 24: w =1.786,loss=0.43797949\n",
      "epoch 25: w =1.790,loss=0.43467349\n",
      "epoch 26: w =1.794,loss=0.43173498\n",
      "epoch 27: w =1.797,loss=0.42905524\n",
      "epoch 28: w =1.801,loss=0.42655879\n",
      "epoch 29: w =1.803,loss=0.42419365\n",
      "epoch 30: w =1.806,loss=0.42192322\n",
      "epoch 31: w =1.808,loss=0.41972223\n",
      "epoch 32: w =1.811,loss=0.41757321\n",
      "epoch 33: w =1.813,loss=0.41546401\n",
      "epoch 34: w =1.815,loss=0.41338608\n",
      "epoch 35: w =1.817,loss=0.41133341\n",
      "epoch 36: w =1.818,loss=0.40930235\n",
      "epoch 37: w =1.820,loss=0.40728936\n",
      "epoch 38: w =1.822,loss=0.40529284\n",
      "epoch 39: w =1.823,loss=0.40331125\n",
      "epoch 40: w =1.825,loss=0.40134361\n",
      "epoch 41: w =1.827,loss=0.39938921\n",
      "epoch 42: w =1.828,loss=0.39744744\n",
      "epoch 43: w =1.830,loss=0.39551798\n",
      "epoch 44: w =1.831,loss=0.39360061\n",
      "epoch 45: w =1.833,loss=0.39169487\n",
      "epoch 46: w =1.834,loss=0.38980097\n",
      "epoch 47: w =1.836,loss=0.38791838\n",
      "epoch 48: w =1.837,loss=0.38604730\n",
      "epoch 49: w =1.838,loss=0.38418731\n",
      "epoch 50: w =1.840,loss=0.38233870\n",
      "epoch 51: w =1.841,loss=0.38050115\n",
      "epoch 52: w =1.843,loss=0.37867451\n",
      "epoch 53: w =1.844,loss=0.37685883\n",
      "epoch 54: w =1.845,loss=0.37505400\n",
      "epoch 55: w =1.847,loss=0.37325999\n",
      "epoch 56: w =1.848,loss=0.37147677\n",
      "epoch 57: w =1.850,loss=0.36970422\n",
      "epoch 58: w =1.851,loss=0.36794218\n",
      "epoch 59: w =1.852,loss=0.36619073\n",
      "epoch 60: w =1.854,loss=0.36444974\n",
      "epoch 61: w =1.855,loss=0.36271906\n",
      "epoch 62: w =1.856,loss=0.36099890\n",
      "epoch 63: w =1.858,loss=0.35928887\n",
      "epoch 64: w =1.859,loss=0.35758919\n",
      "epoch 65: w =1.860,loss=0.35589969\n",
      "epoch 66: w =1.862,loss=0.35422030\n",
      "epoch 67: w =1.863,loss=0.35255077\n",
      "epoch 68: w =1.864,loss=0.35089144\n",
      "epoch 69: w =1.865,loss=0.34924188\n",
      "epoch 70: w =1.867,loss=0.34760216\n",
      "epoch 71: w =1.868,loss=0.34597236\n",
      "epoch 72: w =1.869,loss=0.34435228\n",
      "epoch 73: w =1.871,loss=0.34274185\n",
      "epoch 74: w =1.872,loss=0.34114105\n",
      "epoch 75: w =1.873,loss=0.33954990\n",
      "epoch 76: w =1.874,loss=0.33796820\n",
      "epoch 77: w =1.876,loss=0.33639604\n",
      "epoch 78: w =1.877,loss=0.33483315\n",
      "epoch 79: w =1.878,loss=0.33327961\n",
      "epoch 80: w =1.880,loss=0.33173552\n",
      "epoch 81: w =1.881,loss=0.33020043\n",
      "epoch 82: w =1.882,loss=0.32867455\n",
      "epoch 83: w =1.883,loss=0.32715788\n",
      "epoch 84: w =1.885,loss=0.32565033\n",
      "epoch 85: w =1.886,loss=0.32415172\n",
      "epoch 86: w =1.887,loss=0.32266214\n",
      "epoch 87: w =1.888,loss=0.32118133\n",
      "epoch 88: w =1.890,loss=0.31970948\n",
      "epoch 89: w =1.891,loss=0.31824642\n",
      "epoch 90: w =1.892,loss=0.31679216\n",
      "epoch 91: w =1.893,loss=0.31534642\n",
      "epoch 92: w =1.894,loss=0.31390938\n",
      "epoch 93: w =1.896,loss=0.31248108\n",
      "epoch 94: w =1.897,loss=0.31106120\n",
      "epoch 95: w =1.898,loss=0.30964980\n",
      "epoch 96: w =1.899,loss=0.30824691\n",
      "epoch 97: w =1.900,loss=0.30685231\n",
      "epoch 98: w =1.902,loss=0.30546603\n",
      "epoch 99: w =1.903,loss=0.30408818\n",
      "epoch 100: w =1.904,loss=0.30271843\n",
      "prediction after training:f(5)=9.684\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = model(X)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w ={w[0][0].item():.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={model(X_test).item():.3f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "550ad590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcd92049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[1],[4],[6],[8]],dtype=torch.float32)\n",
    "X_test = torch.tensor([5],dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples)\n",
    "print(n_features)\n",
    "\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f3417e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bee4fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1999e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=-2.658\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6d2b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96656753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =-0.247,loss=45.18228912\n",
      "epoch 2: w =0.058,loss=31.60637283\n",
      "epoch 3: w =0.313,loss=22.18494415\n",
      "epoch 4: w =0.526,loss=15.64622879\n",
      "epoch 5: w =0.704,loss=11.10778236\n",
      "epoch 6: w =0.852,loss=7.95728397\n",
      "epoch 7: w =0.976,loss=5.76986504\n",
      "epoch 8: w =1.079,loss=4.25071287\n",
      "epoch 9: w =1.166,loss=3.19526458\n",
      "epoch 10: w =1.238,loss=2.46157980\n",
      "epoch 11: w =1.299,loss=1.95116639\n",
      "epoch 12: w =1.350,loss=1.59568501\n",
      "epoch 13: w =1.393,loss=1.34771562\n",
      "epoch 14: w =1.429,loss=1.17435431\n",
      "epoch 15: w =1.459,loss=1.05276954\n",
      "epoch 16: w =1.485,loss=0.96711946\n",
      "epoch 17: w =1.506,loss=0.90641117\n",
      "epoch 18: w =1.525,loss=0.86301702\n",
      "epoch 19: w =1.540,loss=0.83164459\n",
      "epoch 20: w =1.554,loss=0.80862129\n",
      "epoch 21: w =1.565,loss=0.79139847\n",
      "epoch 22: w =1.575,loss=0.77820826\n",
      "epoch 23: w =1.584,loss=0.76782346\n",
      "epoch 24: w =1.591,loss=0.75939304\n",
      "epoch 25: w =1.598,loss=0.75232530\n",
      "epoch 26: w =1.604,loss=0.74621111\n",
      "epoch 27: w =1.609,loss=0.74076545\n",
      "epoch 28: w =1.614,loss=0.73579085\n",
      "epoch 29: w =1.618,loss=0.73115027\n",
      "epoch 30: w =1.622,loss=0.72674841\n",
      "epoch 31: w =1.625,loss=0.72251976\n",
      "epoch 32: w =1.628,loss=0.71841794\n",
      "epoch 33: w =1.631,loss=0.71441114\n",
      "epoch 34: w =1.634,loss=0.71047711\n",
      "epoch 35: w =1.637,loss=0.70660079\n",
      "epoch 36: w =1.640,loss=0.70277107\n",
      "epoch 37: w =1.642,loss=0.69898057\n",
      "epoch 38: w =1.644,loss=0.69522417\n",
      "epoch 39: w =1.647,loss=0.69149816\n",
      "epoch 40: w =1.649,loss=0.68779975\n",
      "epoch 41: w =1.651,loss=0.68412751\n",
      "epoch 42: w =1.653,loss=0.68047965\n",
      "epoch 43: w =1.655,loss=0.67685562\n",
      "epoch 44: w =1.657,loss=0.67325401\n",
      "epoch 45: w =1.660,loss=0.66967529\n",
      "epoch 46: w =1.662,loss=0.66611850\n",
      "epoch 47: w =1.664,loss=0.66258329\n",
      "epoch 48: w =1.666,loss=0.65906948\n",
      "epoch 49: w =1.667,loss=0.65557694\n",
      "epoch 50: w =1.669,loss=0.65210563\n",
      "epoch 51: w =1.671,loss=0.64865488\n",
      "epoch 52: w =1.673,loss=0.64522499\n",
      "epoch 53: w =1.675,loss=0.64181560\n",
      "epoch 54: w =1.677,loss=0.63842648\n",
      "epoch 55: w =1.679,loss=0.63505787\n",
      "epoch 56: w =1.681,loss=0.63170946\n",
      "epoch 57: w =1.683,loss=0.62838095\n",
      "epoch 58: w =1.685,loss=0.62507218\n",
      "epoch 59: w =1.686,loss=0.62178355\n",
      "epoch 60: w =1.688,loss=0.61851430\n",
      "epoch 61: w =1.690,loss=0.61526483\n",
      "epoch 62: w =1.692,loss=0.61203450\n",
      "epoch 63: w =1.694,loss=0.60882366\n",
      "epoch 64: w =1.696,loss=0.60563207\n",
      "epoch 65: w =1.697,loss=0.60245931\n",
      "epoch 66: w =1.699,loss=0.59930593\n",
      "epoch 67: w =1.701,loss=0.59617114\n",
      "epoch 68: w =1.703,loss=0.59305513\n",
      "epoch 69: w =1.705,loss=0.58995783\n",
      "epoch 70: w =1.706,loss=0.58687901\n",
      "epoch 71: w =1.708,loss=0.58381844\n",
      "epoch 72: w =1.710,loss=0.58077633\n",
      "epoch 73: w =1.712,loss=0.57775247\n",
      "epoch 74: w =1.713,loss=0.57474655\n",
      "epoch 75: w =1.715,loss=0.57175869\n",
      "epoch 76: w =1.717,loss=0.56878871\n",
      "epoch 77: w =1.719,loss=0.56583637\n",
      "epoch 78: w =1.720,loss=0.56290162\n",
      "epoch 79: w =1.722,loss=0.55998445\n",
      "epoch 80: w =1.724,loss=0.55708504\n",
      "epoch 81: w =1.726,loss=0.55420262\n",
      "epoch 82: w =1.727,loss=0.55133742\n",
      "epoch 83: w =1.729,loss=0.54848957\n",
      "epoch 84: w =1.731,loss=0.54565859\n",
      "epoch 85: w =1.732,loss=0.54284459\n",
      "epoch 86: w =1.734,loss=0.54004759\n",
      "epoch 87: w =1.736,loss=0.53726709\n",
      "epoch 88: w =1.738,loss=0.53450322\n",
      "epoch 89: w =1.739,loss=0.53175598\n",
      "epoch 90: w =1.741,loss=0.52902502\n",
      "epoch 91: w =1.743,loss=0.52631044\n",
      "epoch 92: w =1.744,loss=0.52361220\n",
      "epoch 93: w =1.746,loss=0.52092993\n",
      "epoch 94: w =1.748,loss=0.51826382\n",
      "epoch 95: w =1.749,loss=0.51561362\n",
      "epoch 96: w =1.751,loss=0.51297927\n",
      "epoch 97: w =1.753,loss=0.51036060\n",
      "epoch 98: w =1.754,loss=0.50775766\n",
      "epoch 99: w =1.756,loss=0.50517029\n",
      "epoch 100: w =1.757,loss=0.50259835\n",
      "epoch 101: w =1.759,loss=0.50004196\n",
      "epoch 102: w =1.761,loss=0.49750060\n",
      "epoch 103: w =1.762,loss=0.49497440\n",
      "epoch 104: w =1.764,loss=0.49246353\n",
      "epoch 105: w =1.765,loss=0.48996764\n",
      "epoch 106: w =1.767,loss=0.48748651\n",
      "epoch 107: w =1.769,loss=0.48502037\n",
      "epoch 108: w =1.770,loss=0.48256892\n",
      "epoch 109: w =1.772,loss=0.48013216\n",
      "epoch 110: w =1.773,loss=0.47770992\n",
      "epoch 111: w =1.775,loss=0.47530231\n",
      "epoch 112: w =1.777,loss=0.47290879\n",
      "epoch 113: w =1.778,loss=0.47052988\n",
      "epoch 114: w =1.780,loss=0.46816495\n",
      "epoch 115: w =1.781,loss=0.46581441\n",
      "epoch 116: w =1.783,loss=0.46347776\n",
      "epoch 117: w =1.784,loss=0.46115512\n",
      "epoch 118: w =1.786,loss=0.45884645\n",
      "epoch 119: w =1.787,loss=0.45655137\n",
      "epoch 120: w =1.789,loss=0.45427006\n",
      "epoch 121: w =1.791,loss=0.45200253\n",
      "epoch 122: w =1.792,loss=0.44974855\n",
      "epoch 123: w =1.794,loss=0.44750792\n",
      "epoch 124: w =1.795,loss=0.44528082\n",
      "epoch 125: w =1.797,loss=0.44306701\n",
      "epoch 126: w =1.798,loss=0.44086629\n",
      "epoch 127: w =1.800,loss=0.43867892\n",
      "epoch 128: w =1.801,loss=0.43650451\n",
      "epoch 129: w =1.803,loss=0.43434310\n",
      "epoch 130: w =1.804,loss=0.43219462\n",
      "epoch 131: w =1.806,loss=0.43005905\n",
      "epoch 132: w =1.807,loss=0.42793623\n",
      "epoch 133: w =1.809,loss=0.42582613\n",
      "epoch 134: w =1.810,loss=0.42372856\n",
      "epoch 135: w =1.811,loss=0.42164353\n",
      "epoch 136: w =1.813,loss=0.41957107\n",
      "epoch 137: w =1.814,loss=0.41751093\n",
      "epoch 138: w =1.816,loss=0.41546315\n",
      "epoch 139: w =1.817,loss=0.41342762\n",
      "epoch 140: w =1.819,loss=0.41140404\n",
      "epoch 141: w =1.820,loss=0.40939280\n",
      "epoch 142: w =1.822,loss=0.40739354\n",
      "epoch 143: w =1.823,loss=0.40540621\n",
      "epoch 144: w =1.824,loss=0.40343067\n",
      "epoch 145: w =1.826,loss=0.40146708\n",
      "epoch 146: w =1.827,loss=0.39951515\n",
      "epoch 147: w =1.829,loss=0.39757487\n",
      "epoch 148: w =1.830,loss=0.39564630\n",
      "epoch 149: w =1.832,loss=0.39372915\n",
      "epoch 150: w =1.833,loss=0.39182365\n",
      "epoch 151: w =1.834,loss=0.38992932\n",
      "epoch 152: w =1.836,loss=0.38804638\n",
      "epoch 153: w =1.837,loss=0.38617483\n",
      "epoch 154: w =1.839,loss=0.38431436\n",
      "epoch 155: w =1.840,loss=0.38246506\n",
      "epoch 156: w =1.841,loss=0.38062668\n",
      "epoch 157: w =1.843,loss=0.37879947\n",
      "epoch 158: w =1.844,loss=0.37698305\n",
      "epoch 159: w =1.845,loss=0.37517750\n",
      "epoch 160: w =1.847,loss=0.37338287\n",
      "epoch 161: w =1.848,loss=0.37159887\n",
      "epoch 162: w =1.849,loss=0.36982554\n",
      "epoch 163: w =1.851,loss=0.36806276\n",
      "epoch 164: w =1.852,loss=0.36631069\n",
      "epoch 165: w =1.853,loss=0.36456892\n",
      "epoch 166: w =1.855,loss=0.36283761\n",
      "epoch 167: w =1.856,loss=0.36111665\n",
      "epoch 168: w =1.857,loss=0.35940602\n",
      "epoch 169: w =1.859,loss=0.35770571\n",
      "epoch 170: w =1.860,loss=0.35601538\n",
      "epoch 171: w =1.861,loss=0.35433540\n",
      "epoch 172: w =1.863,loss=0.35266519\n",
      "epoch 173: w =1.864,loss=0.35100508\n",
      "epoch 174: w =1.865,loss=0.34935483\n",
      "epoch 175: w =1.867,loss=0.34771454\n",
      "epoch 176: w =1.868,loss=0.34608397\n",
      "epoch 177: w =1.869,loss=0.34446320\n",
      "epoch 178: w =1.871,loss=0.34285218\n",
      "epoch 179: w =1.872,loss=0.34125069\n",
      "epoch 180: w =1.873,loss=0.33965892\n",
      "epoch 181: w =1.874,loss=0.33807662\n",
      "epoch 182: w =1.876,loss=0.33650365\n",
      "epoch 183: w =1.877,loss=0.33494011\n",
      "epoch 184: w =1.878,loss=0.33338603\n",
      "epoch 185: w =1.879,loss=0.33184114\n",
      "epoch 186: w =1.881,loss=0.33030552\n",
      "epoch 187: w =1.882,loss=0.32877913\n",
      "epoch 188: w =1.883,loss=0.32726172\n",
      "epoch 189: w =1.884,loss=0.32575348\n",
      "epoch 190: w =1.886,loss=0.32425427\n",
      "epoch 191: w =1.887,loss=0.32276410\n",
      "epoch 192: w =1.888,loss=0.32128268\n",
      "epoch 193: w =1.889,loss=0.31981030\n",
      "epoch 194: w =1.891,loss=0.31834644\n",
      "epoch 195: w =1.892,loss=0.31689161\n",
      "epoch 196: w =1.893,loss=0.31544542\n",
      "epoch 197: w =1.894,loss=0.31400779\n",
      "epoch 198: w =1.896,loss=0.31257874\n",
      "epoch 199: w =1.897,loss=0.31115836\n",
      "epoch 200: w =1.898,loss=0.30974638\n",
      "epoch 201: w =1.899,loss=0.30834284\n",
      "epoch 202: w =1.900,loss=0.30694780\n",
      "epoch 203: w =1.902,loss=0.30556107\n",
      "epoch 204: w =1.903,loss=0.30418244\n",
      "epoch 205: w =1.904,loss=0.30281219\n",
      "epoch 206: w =1.905,loss=0.30145016\n",
      "epoch 207: w =1.906,loss=0.30009627\n",
      "epoch 208: w =1.907,loss=0.29875052\n",
      "epoch 209: w =1.909,loss=0.29741278\n",
      "epoch 210: w =1.910,loss=0.29608297\n",
      "epoch 211: w =1.911,loss=0.29476115\n",
      "epoch 212: w =1.912,loss=0.29344726\n",
      "epoch 213: w =1.913,loss=0.29214117\n",
      "epoch 214: w =1.914,loss=0.29084283\n",
      "epoch 215: w =1.916,loss=0.28955230\n",
      "epoch 216: w =1.917,loss=0.28826967\n",
      "epoch 217: w =1.918,loss=0.28699452\n",
      "epoch 218: w =1.919,loss=0.28572702\n",
      "epoch 219: w =1.920,loss=0.28446704\n",
      "epoch 220: w =1.921,loss=0.28321472\n",
      "epoch 221: w =1.923,loss=0.28196982\n",
      "epoch 222: w =1.924,loss=0.28073248\n",
      "epoch 223: w =1.925,loss=0.27950233\n",
      "epoch 224: w =1.926,loss=0.27827975\n",
      "epoch 225: w =1.927,loss=0.27706435\n",
      "epoch 226: w =1.928,loss=0.27585614\n",
      "epoch 227: w =1.929,loss=0.27465537\n",
      "epoch 228: w =1.930,loss=0.27346164\n",
      "epoch 229: w =1.931,loss=0.27227500\n",
      "epoch 230: w =1.933,loss=0.27109560\n",
      "epoch 231: w =1.934,loss=0.26992309\n",
      "epoch 232: w =1.935,loss=0.26875773\n",
      "epoch 233: w =1.936,loss=0.26759925\n",
      "epoch 234: w =1.937,loss=0.26644781\n",
      "epoch 235: w =1.938,loss=0.26530313\n",
      "epoch 236: w =1.939,loss=0.26416537\n",
      "epoch 237: w =1.940,loss=0.26303428\n",
      "epoch 238: w =1.941,loss=0.26191008\n",
      "epoch 239: w =1.942,loss=0.26079261\n",
      "epoch 240: w =1.943,loss=0.25968179\n",
      "epoch 241: w =1.944,loss=0.25857750\n",
      "epoch 242: w =1.946,loss=0.25748000\n",
      "epoch 243: w =1.947,loss=0.25638899\n",
      "epoch 244: w =1.948,loss=0.25530443\n",
      "epoch 245: w =1.949,loss=0.25422645\n",
      "epoch 246: w =1.950,loss=0.25315490\n",
      "epoch 247: w =1.951,loss=0.25208980\n",
      "epoch 248: w =1.952,loss=0.25103089\n",
      "epoch 249: w =1.953,loss=0.24997850\n",
      "epoch 250: w =1.954,loss=0.24893235\n",
      "epoch 251: w =1.955,loss=0.24789238\n",
      "epoch 252: w =1.956,loss=0.24685875\n",
      "epoch 253: w =1.957,loss=0.24583124\n",
      "epoch 254: w =1.958,loss=0.24480988\n",
      "epoch 255: w =1.959,loss=0.24379461\n",
      "epoch 256: w =1.960,loss=0.24278536\n",
      "epoch 257: w =1.961,loss=0.24178223\n",
      "epoch 258: w =1.962,loss=0.24078502\n",
      "epoch 259: w =1.963,loss=0.23979387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 260: w =1.964,loss=0.23880860\n",
      "epoch 261: w =1.965,loss=0.23782918\n",
      "epoch 262: w =1.966,loss=0.23685569\n",
      "epoch 263: w =1.967,loss=0.23588796\n",
      "epoch 264: w =1.968,loss=0.23492600\n",
      "epoch 265: w =1.969,loss=0.23396984\n",
      "epoch 266: w =1.970,loss=0.23301941\n",
      "epoch 267: w =1.971,loss=0.23207459\n",
      "epoch 268: w =1.972,loss=0.23113549\n",
      "epoch 269: w =1.973,loss=0.23020200\n",
      "epoch 270: w =1.974,loss=0.22927405\n",
      "epoch 271: w =1.975,loss=0.22835173\n",
      "epoch 272: w =1.976,loss=0.22743480\n",
      "epoch 273: w =1.977,loss=0.22652347\n",
      "epoch 274: w =1.978,loss=0.22561750\n",
      "epoch 275: w =1.979,loss=0.22471701\n",
      "epoch 276: w =1.980,loss=0.22382183\n",
      "epoch 277: w =1.981,loss=0.22293204\n",
      "epoch 278: w =1.982,loss=0.22204761\n",
      "epoch 279: w =1.983,loss=0.22116846\n",
      "epoch 280: w =1.984,loss=0.22029455\n",
      "epoch 281: w =1.985,loss=0.21942583\n",
      "epoch 282: w =1.986,loss=0.21856228\n",
      "epoch 283: w =1.987,loss=0.21770397\n",
      "epoch 284: w =1.987,loss=0.21685073\n",
      "epoch 285: w =1.988,loss=0.21600269\n",
      "epoch 286: w =1.989,loss=0.21515964\n",
      "epoch 287: w =1.990,loss=0.21432167\n",
      "epoch 288: w =1.991,loss=0.21348868\n",
      "epoch 289: w =1.992,loss=0.21266069\n",
      "epoch 290: w =1.993,loss=0.21183765\n",
      "epoch 291: w =1.994,loss=0.21101949\n",
      "epoch 292: w =1.995,loss=0.21020624\n",
      "epoch 293: w =1.996,loss=0.20939790\n",
      "epoch 294: w =1.997,loss=0.20859432\n",
      "epoch 295: w =1.998,loss=0.20779563\n",
      "epoch 296: w =1.999,loss=0.20700161\n",
      "epoch 297: w =1.999,loss=0.20621239\n",
      "epoch 298: w =2.000,loss=0.20542786\n",
      "epoch 299: w =2.001,loss=0.20464811\n",
      "epoch 300: w =2.002,loss=0.20387301\n",
      "epoch 301: w =2.003,loss=0.20310244\n",
      "epoch 302: w =2.004,loss=0.20233656\n",
      "epoch 303: w =2.005,loss=0.20157519\n",
      "epoch 304: w =2.006,loss=0.20081842\n",
      "epoch 305: w =2.007,loss=0.20006621\n",
      "epoch 306: w =2.007,loss=0.19931845\n",
      "epoch 307: w =2.008,loss=0.19857518\n",
      "epoch 308: w =2.009,loss=0.19783635\n",
      "epoch 309: w =2.010,loss=0.19710195\n",
      "epoch 310: w =2.011,loss=0.19637190\n",
      "epoch 311: w =2.012,loss=0.19564623\n",
      "epoch 312: w =2.013,loss=0.19492492\n",
      "epoch 313: w =2.014,loss=0.19420791\n",
      "epoch 314: w =2.014,loss=0.19349517\n",
      "epoch 315: w =2.015,loss=0.19278672\n",
      "epoch 316: w =2.016,loss=0.19208245\n",
      "epoch 317: w =2.017,loss=0.19138247\n",
      "epoch 318: w =2.018,loss=0.19068667\n",
      "epoch 319: w =2.019,loss=0.18999496\n",
      "epoch 320: w =2.019,loss=0.18930744\n",
      "epoch 321: w =2.020,loss=0.18862405\n",
      "epoch 322: w =2.021,loss=0.18794464\n",
      "epoch 323: w =2.022,loss=0.18726940\n",
      "epoch 324: w =2.023,loss=0.18659811\n",
      "epoch 325: w =2.024,loss=0.18593094\n",
      "epoch 326: w =2.024,loss=0.18526769\n",
      "epoch 327: w =2.025,loss=0.18460841\n",
      "epoch 328: w =2.026,loss=0.18395311\n",
      "epoch 329: w =2.027,loss=0.18330167\n",
      "epoch 330: w =2.028,loss=0.18265419\n",
      "epoch 331: w =2.029,loss=0.18201052\n",
      "epoch 332: w =2.029,loss=0.18137074\n",
      "epoch 333: w =2.030,loss=0.18073477\n",
      "epoch 334: w =2.031,loss=0.18010260\n",
      "epoch 335: w =2.032,loss=0.17947420\n",
      "epoch 336: w =2.033,loss=0.17884953\n",
      "epoch 337: w =2.033,loss=0.17822863\n",
      "epoch 338: w =2.034,loss=0.17761146\n",
      "epoch 339: w =2.035,loss=0.17699800\n",
      "epoch 340: w =2.036,loss=0.17638816\n",
      "epoch 341: w =2.037,loss=0.17578200\n",
      "epoch 342: w =2.037,loss=0.17517936\n",
      "epoch 343: w =2.038,loss=0.17458041\n",
      "epoch 344: w =2.039,loss=0.17398505\n",
      "epoch 345: w =2.040,loss=0.17339326\n",
      "epoch 346: w =2.040,loss=0.17280498\n",
      "epoch 347: w =2.041,loss=0.17222025\n",
      "epoch 348: w =2.042,loss=0.17163892\n",
      "epoch 349: w =2.043,loss=0.17106116\n",
      "epoch 350: w =2.044,loss=0.17048682\n",
      "epoch 351: w =2.044,loss=0.16991591\n",
      "epoch 352: w =2.045,loss=0.16934846\n",
      "epoch 353: w =2.046,loss=0.16878435\n",
      "epoch 354: w =2.047,loss=0.16822360\n",
      "epoch 355: w =2.047,loss=0.16766623\n",
      "epoch 356: w =2.048,loss=0.16711222\n",
      "epoch 357: w =2.049,loss=0.16656141\n",
      "epoch 358: w =2.050,loss=0.16601406\n",
      "epoch 359: w =2.050,loss=0.16546986\n",
      "epoch 360: w =2.051,loss=0.16492897\n",
      "epoch 361: w =2.052,loss=0.16439135\n",
      "epoch 362: w =2.053,loss=0.16385688\n",
      "epoch 363: w =2.053,loss=0.16332562\n",
      "epoch 364: w =2.054,loss=0.16279756\n",
      "epoch 365: w =2.055,loss=0.16227259\n",
      "epoch 366: w =2.056,loss=0.16175084\n",
      "epoch 367: w =2.056,loss=0.16123217\n",
      "epoch 368: w =2.057,loss=0.16071655\n",
      "epoch 369: w =2.058,loss=0.16020407\n",
      "epoch 370: w =2.059,loss=0.15969467\n",
      "epoch 371: w =2.059,loss=0.15918829\n",
      "epoch 372: w =2.060,loss=0.15868495\n",
      "epoch 373: w =2.061,loss=0.15818459\n",
      "epoch 374: w =2.061,loss=0.15768731\n",
      "epoch 375: w =2.062,loss=0.15719286\n",
      "epoch 376: w =2.063,loss=0.15670148\n",
      "epoch 377: w =2.064,loss=0.15621299\n",
      "epoch 378: w =2.064,loss=0.15572749\n",
      "epoch 379: w =2.065,loss=0.15524478\n",
      "epoch 380: w =2.066,loss=0.15476501\n",
      "epoch 381: w =2.066,loss=0.15428810\n",
      "epoch 382: w =2.067,loss=0.15381405\n",
      "epoch 383: w =2.068,loss=0.15334281\n",
      "epoch 384: w =2.068,loss=0.15287448\n",
      "epoch 385: w =2.069,loss=0.15240884\n",
      "epoch 386: w =2.070,loss=0.15194604\n",
      "epoch 387: w =2.071,loss=0.15148596\n",
      "epoch 388: w =2.071,loss=0.15102868\n",
      "epoch 389: w =2.072,loss=0.15057413\n",
      "epoch 390: w =2.073,loss=0.15012227\n",
      "epoch 391: w =2.073,loss=0.14967313\n",
      "epoch 392: w =2.074,loss=0.14922662\n",
      "epoch 393: w =2.075,loss=0.14878291\n",
      "epoch 394: w =2.075,loss=0.14834175\n",
      "epoch 395: w =2.076,loss=0.14790326\n",
      "epoch 396: w =2.077,loss=0.14746737\n",
      "epoch 397: w =2.077,loss=0.14703408\n",
      "epoch 398: w =2.078,loss=0.14660345\n",
      "epoch 399: w =2.079,loss=0.14617532\n",
      "epoch 400: w =2.079,loss=0.14574976\n",
      "epoch 401: w =2.080,loss=0.14532675\n",
      "epoch 402: w =2.081,loss=0.14490624\n",
      "epoch 403: w =2.081,loss=0.14448833\n",
      "epoch 404: w =2.082,loss=0.14407286\n",
      "epoch 405: w =2.083,loss=0.14365989\n",
      "epoch 406: w =2.083,loss=0.14324938\n",
      "epoch 407: w =2.084,loss=0.14284137\n",
      "epoch 408: w =2.085,loss=0.14243570\n",
      "epoch 409: w =2.085,loss=0.14203253\n",
      "epoch 410: w =2.086,loss=0.14163178\n",
      "epoch 411: w =2.086,loss=0.14123340\n",
      "epoch 412: w =2.087,loss=0.14083740\n",
      "epoch 413: w =2.088,loss=0.14044371\n",
      "epoch 414: w =2.088,loss=0.14005248\n",
      "epoch 415: w =2.089,loss=0.13966352\n",
      "epoch 416: w =2.090,loss=0.13927688\n",
      "epoch 417: w =2.090,loss=0.13889261\n",
      "epoch 418: w =2.091,loss=0.13851060\n",
      "epoch 419: w =2.092,loss=0.13813090\n",
      "epoch 420: w =2.092,loss=0.13775344\n",
      "epoch 421: w =2.093,loss=0.13737825\n",
      "epoch 422: w =2.093,loss=0.13700527\n",
      "epoch 423: w =2.094,loss=0.13663457\n",
      "epoch 424: w =2.095,loss=0.13626605\n",
      "epoch 425: w =2.095,loss=0.13589974\n",
      "epoch 426: w =2.096,loss=0.13553564\n",
      "epoch 427: w =2.096,loss=0.13517368\n",
      "epoch 428: w =2.097,loss=0.13481393\n",
      "epoch 429: w =2.098,loss=0.13445631\n",
      "epoch 430: w =2.098,loss=0.13410082\n",
      "epoch 431: w =2.099,loss=0.13374744\n",
      "epoch 432: w =2.099,loss=0.13339625\n",
      "epoch 433: w =2.100,loss=0.13304707\n",
      "epoch 434: w =2.101,loss=0.13270006\n",
      "epoch 435: w =2.101,loss=0.13235506\n",
      "epoch 436: w =2.102,loss=0.13201216\n",
      "epoch 437: w =2.102,loss=0.13167128\n",
      "epoch 438: w =2.103,loss=0.13133243\n",
      "epoch 439: w =2.104,loss=0.13099566\n",
      "epoch 440: w =2.104,loss=0.13066085\n",
      "epoch 441: w =2.105,loss=0.13032806\n",
      "epoch 442: w =2.105,loss=0.12999728\n",
      "epoch 443: w =2.106,loss=0.12966844\n",
      "epoch 444: w =2.107,loss=0.12934160\n",
      "epoch 445: w =2.107,loss=0.12901673\n",
      "epoch 446: w =2.108,loss=0.12869373\n",
      "epoch 447: w =2.108,loss=0.12837267\n",
      "epoch 448: w =2.109,loss=0.12805365\n",
      "epoch 449: w =2.109,loss=0.12773643\n",
      "epoch 450: w =2.110,loss=0.12742111\n",
      "epoch 451: w =2.111,loss=0.12710769\n",
      "epoch 452: w =2.111,loss=0.12679616\n",
      "epoch 453: w =2.112,loss=0.12648647\n",
      "epoch 454: w =2.112,loss=0.12617864\n",
      "epoch 455: w =2.113,loss=0.12587269\n",
      "epoch 456: w =2.113,loss=0.12556855\n",
      "epoch 457: w =2.114,loss=0.12526616\n",
      "epoch 458: w =2.115,loss=0.12496563\n",
      "epoch 459: w =2.115,loss=0.12466690\n",
      "epoch 460: w =2.116,loss=0.12436997\n",
      "epoch 461: w =2.116,loss=0.12407481\n",
      "epoch 462: w =2.117,loss=0.12378137\n",
      "epoch 463: w =2.117,loss=0.12348973\n",
      "epoch 464: w =2.118,loss=0.12319984\n",
      "epoch 465: w =2.118,loss=0.12291168\n",
      "epoch 466: w =2.119,loss=0.12262521\n",
      "epoch 467: w =2.119,loss=0.12234041\n",
      "epoch 468: w =2.120,loss=0.12205737\n",
      "epoch 469: w =2.121,loss=0.12177604\n",
      "epoch 470: w =2.121,loss=0.12149636\n",
      "epoch 471: w =2.122,loss=0.12121841\n",
      "epoch 472: w =2.122,loss=0.12094203\n",
      "epoch 473: w =2.123,loss=0.12066738\n",
      "epoch 474: w =2.123,loss=0.12039433\n",
      "epoch 475: w =2.124,loss=0.12012293\n",
      "epoch 476: w =2.124,loss=0.11985312\n",
      "epoch 477: w =2.125,loss=0.11958496\n",
      "epoch 478: w =2.125,loss=0.11931842\n",
      "epoch 479: w =2.126,loss=0.11905344\n",
      "epoch 480: w =2.126,loss=0.11879003\n",
      "epoch 481: w =2.127,loss=0.11852822\n",
      "epoch 482: w =2.127,loss=0.11826800\n",
      "epoch 483: w =2.128,loss=0.11800931\n",
      "epoch 484: w =2.128,loss=0.11775216\n",
      "epoch 485: w =2.129,loss=0.11749651\n",
      "epoch 486: w =2.129,loss=0.11724249\n",
      "epoch 487: w =2.130,loss=0.11698988\n",
      "epoch 488: w =2.130,loss=0.11673886\n",
      "epoch 489: w =2.131,loss=0.11648931\n",
      "epoch 490: w =2.131,loss=0.11624122\n",
      "epoch 491: w =2.132,loss=0.11599468\n",
      "epoch 492: w =2.132,loss=0.11574961\n",
      "epoch 493: w =2.133,loss=0.11550593\n",
      "epoch 494: w =2.133,loss=0.11526375\n",
      "epoch 495: w =2.134,loss=0.11502301\n",
      "epoch 496: w =2.134,loss=0.11478375\n",
      "epoch 497: w =2.135,loss=0.11454585\n",
      "epoch 498: w =2.135,loss=0.11430945\n",
      "epoch 499: w =2.136,loss=0.11407440\n",
      "epoch 500: w =2.136,loss=0.11384076\n",
      "prediction after training:f(5)=10.163\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = model.forward(X)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w ={w[0][0].item():.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={model(X_test).item():.3f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71c692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
